{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632c20ed",
   "metadata": {
    "id": "632c20ed"
   },
   "source": [
    "# Reading and Writing Files in Python for Machine Learning\n",
    "This notebook demonstrates reading various file types commonly used in Machine Learning workflows using Python,\n",
    "and also how to write DataFrames to Excel files with explanations of key hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05284e5d-1c4f-41f3-a7d7-c749f149cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To know the current working directory where you can store the input data files used in this notebook.\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "# Set the new directory path if required\n",
    "os.chdir(\"C:/Users/Username/Downloads/Folders/Datasets and Python Notebook for Handling Files\")\n",
    "\n",
    "# Confirm the change\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18806eab",
   "metadata": {
    "id": "18806eab"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93018c2c",
   "metadata": {
    "id": "93018c2c"
   },
   "source": [
    "# 1. Reading CSV files\n",
    "\n",
    "Use `pandas.read_csv()` for CSV files.\n",
    "\n",
    "Common hyperparameters include\n",
    "\n",
    "#### Basic Structure & Data Loading\n",
    "|Parameter|Purpose|\n",
    "|---------|-------|\n",
    "|filepath_or_buffer|Path to the CSV file (or URL). Core input.|\n",
    "|sep|Delimiter used in the file (e.g., ',', '\\t', ';').|\n",
    "|header|\tRow number(s) to use as the column names. Commonly 0.|\n",
    "|names|\tIf no header or to assign custom column names.|\n",
    "|index_col|\tColumn(s) to use as row labels (indexes). Important for time series or grouped data.|\n",
    "|na_values|\tAdditional strings to recognize as NA/NaN (e.g., ['NA', '?', '--']).|\n",
    "|dtype|Set data types explicitly (e.g., {'col1': 'float32'}) â€” avoids type inference and saves memory.|\n",
    "|usecols|Select only certain columns (helps with memory/performance).|\n",
    "|nrows|Read only first N rows (great for previewing).|\n",
    "|parse_dates|\tConvert columns to datetime. Can pass list of columns or dict.|\n",
    "|encoding|Specify file encoding (e.g., 'utf-8', 'latin1'). Critical for non-English data.|\n",
    "|on_bad_lines|Skip or warn on malformed lines.|\n",
    "|skiprows|Skip specified number of lines or specific line indices. Useful for metadata removal.|\n",
    "|skip_blank_lines|Skip over blank lines. Prevents parsing issues.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4347f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc4347f2",
    "outputId": "02739ab3-650b-4521-82ae-0bfedee420cc"
   },
   "outputs": [],
   "source": [
    "# Example CSV read\n",
    "import pandas as pd\n",
    "csv_file_path = 'popularity.csv'  # Replace with your CSV file path\n",
    "df_csv = pd.read_csv(csv_file_path, sep=',', header=0, usecols=None, dtype=None, nrows=None)\n",
    "print(df_csv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FPuNHMfRwJY1",
   "metadata": {
    "id": "FPuNHMfRwJY1"
   },
   "source": [
    "```\n",
    "pd.read_csv(\n",
    "    'Datasets/popularity.csv',\n",
    "    sep=',',\n",
    "    header=0,\n",
    "    na_values=['NA', 'N/A', '--'],\n",
    "    dtype={'id': int, 'value': float},\n",
    "    #parse_dates=['date'],\n",
    "    usecols=['id', 'value', 'date'],\n",
    "    encoding='utf-8'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vlmrU4C2wRof",
   "metadata": {
    "id": "vlmrU4C2wRof"
   },
   "source": [
    "# 2. Writing DataFrames to CSV files\n",
    "\n",
    "### Key parameters:\n",
    "\n",
    "|Parameter|Purpose|\n",
    "|---------|-------|\n",
    "|index=False: | Prevents writing the DataFrame's index (row numbers) to the CSV file. If you don't want the row index to appear in the CSV (which is typically unnecessary in most use cases), setting index=False ensures the file is clean and free from unnecessary data.|\n",
    "|header=True: | Specifies whether or not to write column names (headers) to the CSV file. Setting header=True ensures that the first row of the CSV file contains the column names, which helps when reading the file back into a DataFrame (or when sharing the file, for clarity).|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5BYuGckwTRG",
   "metadata": {
    "id": "c5BYuGckwTRG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dummy transactional dataset\n",
    "data = {\n",
    "    'Transaction_ID': [101, 102, 103, 104, 105],\n",
    "    'Customer_Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Monitor'],\n",
    "    'Amount': [1200, 800, 450, 200, 300],\n",
    "    'Date': ['2025-06-01', '2025-06-02', '2025-06-03', '2025-06-04', '2025-06-05']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv('transaction_data.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stH18Bk-wbVz",
   "metadata": {
    "id": "stH18Bk-wbVz"
   },
   "source": [
    "# 3. Reading Excel files\n",
    "- Use `pandas.read_excel()`.\n",
    "- Important hyperparameters: `sheet_name`, `header`, `usecols`, `skiprows`.\n",
    "\n",
    "|Parameter|Purpose|\n",
    "|---------|-------|\n",
    "|io|Path to Excel file or file-like object (mandatory).|\n",
    "|sheet_name|\tName or index of sheet(s) to read. Use None to read all sheets.|\n",
    "|header|\tRow(s) to use as the column names (default is 0). Use None if no header.|\n",
    "|names|\tList of column names to use (overrides header). Useful for renaming.|\n",
    "|usecols|\tSelect subset of columns (by label or Excel-style letters).|\n",
    "|skiprows|\tNumber of rows (or list of rows) to skip at the start.|\n",
    "|nrows|\tLimit the number of rows to read (useful for sampling).|\n",
    "|dtype|\tSpecify column data types. Prevents incorrect automatic inference.|\n",
    "|na_values|\tAdditional strings to recognize as NaN.|\n",
    "|engine|\tEngine to use: \"openpyxl\" (default), \"xlrd\", or \"odf\".|\n",
    "|index_col|\tColumn(s) to set as index.|\n",
    "|parse_dates|\tAutomatically parse dates. Useful for time series data.|\n",
    "|skipfooter|\tSkip rows at the end of the sheet.|\n",
    "|squeeze|\tIf the parsed data only contains one column, return a Series instead.|\n",
    "\n",
    "\n",
    "### Example Excel read\n",
    "```\n",
    "excel_file_path = 'SuperStoreUS-2015.xlsx'  # Replace with your Excel file path\n",
    "df_excel = pd.read_excel(excel_file_path, sheet_name=0, header=0, usecols=None, skiprows=0)\n",
    "print(df_excel.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GcpWGqQEwh3J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "GcpWGqQEwh3J",
    "outputId": "072f2d8e-b42e-47f4-e197-f2d40a8306bd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"SuperStoreUS-2015.xlsx\",\n",
    "                   sheet_name=\"Orders\",\n",
    "                   usecols=[\"Region\",\"Sales\",\"Profit\"],\n",
    "                   skiprows=0,\n",
    "                   nrows=100,\n",
    "                   dtype={\"Region\": str, \"Sales\": float},\n",
    "                   na_values=[\"NA\", \"missing\"],\n",
    "                   #parse_dates=[\"Order Date\"]\n",
    "                   )\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mFRhIGIVwza5",
   "metadata": {
    "id": "mFRhIGIVwza5"
   },
   "source": [
    "# 4. Writing DataFrames to Excel files\n",
    "- Use `DataFrame.to_excel()` to write data to Excel files.\n",
    "- Important hyperparameters include:\n",
    "  \n",
    "|Parameter|Purpose|\n",
    "|---------|-------|  \n",
    "|`sheet_name`|Name of the sheet to write to.|\n",
    "|`index`|Whether to write row index.|\n",
    "|`header`|Whether to write column headers.|\n",
    "|`startrow`|Row position to start writing data.|\n",
    "|`engine`|Excel writer engine, e.g., 'openpyxl'.|\n",
    "\n",
    "Use `pd.ExcelWriter` for writing multiple sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375fDw9w3Vm",
   "metadata": {
    "id": "7375fDw9w3Vm"
   },
   "outputs": [],
   "source": [
    "# Let us create 3 dataframes, and then write them to excel files in respective sheets\n",
    "import pandas as pd\n",
    "# Create a dummy transactional dataset\n",
    "data = {\n",
    "    'Transaction_ID': [101, 102, 103, 104, 105],\n",
    "    'Customer_Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Monitor'],\n",
    "    'Amount': [1200, 800, 450, 200, 300],\n",
    "    'Date': ['2025-06-01', '2025-06-02', '2025-06-03', '2025-06-04', '2025-06-05']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vRd_dTeXw_kg",
   "metadata": {
    "id": "vRd_dTeXw_kg"
   },
   "outputs": [],
   "source": [
    "# Writing a single DataFrame to Excel\n",
    "output_excel_path_1 = 'transactions.xlsx'  # Output Excel file path\n",
    "\n",
    "# Writing CSV DataFrame to Excel\n",
    "df.to_excel(\n",
    "    output_excel_path_1,\n",
    "    sheet_name='df_data',      # Sheet name in Excel\n",
    "    index=False,                # Do not write DataFrame index\n",
    "    header=True,                # Write column headers\n",
    "    startrow=0,                 # Start writing at first row\n",
    "    engine='openpyxl'           # Use openpyxl engine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1GINAOPxBAw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1GINAOPxBAw",
    "outputId": "2e440548-6479-4843-ce4b-36d438b3461b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Simulated data that might come from an Excel file\n",
    "data = {\n",
    "    'EmployeeID': [101, 102, 103],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'Salary': [50000, 60000, 70000]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df_excel = pd.DataFrame(data)\n",
    "\n",
    "print(df_excel)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data simulating JSON records\n",
    "data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"skills\": \"Python, Excel\"},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"skills\": \"Java, SQL\"},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"skills\": \"JavaScript, HTML\"}\n",
    "]\n",
    "\n",
    "# Convert JSON-like list of dicts to DataFrame\n",
    "df_json = pd.DataFrame(data)\n",
    "\n",
    "print(df_json)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Simulated SQL query result\n",
    "data = {\n",
    "    'OrderID': [2001, 2002, 2003],\n",
    "    'Customer': ['John Doe', 'Jane Smith', 'Alice Brown'],\n",
    "    'TotalAmount': [150.75, 250.00, 99.99]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df_sql = pd.DataFrame(data)\n",
    "\n",
    "print(df_sql)\n",
    "\n",
    "\n",
    "#output_excel_path_2 = '/content/output_data_2.xlsx'  # Output Excel file path\n",
    "# Writing multiple DataFrames to different sheets in the same Excel file\n",
    "with pd.ExcelWriter('output_data_2.xlsx', engine='openpyxl', mode='w') as writer:\n",
    "    df_excel.to_excel(writer, sheet_name='Excel_Data', index=False)\n",
    "    df_json.to_excel(writer, sheet_name='JSON_Data', index=False)\n",
    "    df_sql.to_excel(writer, sheet_name='SQL_Data', index=True)  # SQL DataFrame may have index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cWwSlU7vxnAf",
   "metadata": {
    "id": "cWwSlU7vxnAf"
   },
   "source": [
    "# 5. Reading Text Files\n",
    "\n",
    "To read text files in pandas, the function you use depends on the structure of the text file. There are multiple functions to read a text file, but only 2 techniques are covered in the session. The rest of the techniques are for your reference. Most commonly used pandas functions for different types of text files are:\n",
    "\n",
    "### 1. pandas.read_csv()\n",
    "- We can use this when the text file is comma-separated or has a regular delimiter (CSV, TSV, etc.).\n",
    "-You can customize the delimiter using the sep parameter.\n",
    "\n",
    "|Parameter|Purpose|\n",
    "|---------|-------|\n",
    "|filename.txt|As the name suggests it is the name of the text file from which we want to read data.|\n",
    "|sep|It is a separator field. In the text file, we use the space character(' ') as the separator.|\n",
    "|header|This is an optional field. By default, it will take the first line of the text file as a header. If we use header=None then it will create the header|\n",
    "|names|We can assign column names while importing the text file by using the names argument.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wb22hF4lxBLw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wb22hF4lxBLw",
    "outputId": "6c9bc6dd-dca1-4d6f-ad4a-c492949da1bb"
   },
   "outputs": [],
   "source": [
    "with open('Text File 1.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kfVKuOvkxBOl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "kfVKuOvkxBOl",
    "outputId": "c509c980-79d0-4c1a-f157-9a0ecde85793"
   },
   "outputs": [],
   "source": [
    "#Example:\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Text File 1.txt', sep=',')  # comma-separated values\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LpmeZWqKxx6J",
   "metadata": {
    "id": "LpmeZWqKxx6J"
   },
   "source": [
    "### 2. pandas.read_table()\n",
    "- We can use this when the file is tab-delimited or uses other consistent delimiters.\n",
    "- Default delimiter: \\t (tab).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hItCwjAzxBR3",
   "metadata": {
    "id": "hItCwjAzxBR3"
   },
   "outputs": [],
   "source": [
    "with open('employees.txt', 'w') as f:\n",
    "    f.write(\"Name\\tDepartment\\tSalary\\n\")\n",
    "    f.write(\"Alice\\tHR\\t50000\\n\")\n",
    "    f.write(\"Bob\\tEngineering\\t65000\\n\")\n",
    "    f.write(\"Carol\\tMarketing\\t55000\\n\")\n",
    "\n",
    "df = pd.read_table('employees.txt')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OusXbFDHxBUp",
   "metadata": {
    "id": "OusXbFDHxBUp"
   },
   "outputs": [],
   "source": [
    "#Example:\n",
    "df = pd.read_table('Text File 2.txt')  # assumes tab-delimited text\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZZfAhfh3x7Nf",
   "metadata": {
    "id": "ZZfAhfh3x7Nf"
   },
   "source": [
    "### 3. pandas.read_fwf()\n",
    "-We can use this when the file is fixed-width formatted (columns are aligned with fixed widths).\n",
    "\n",
    "Example:\n",
    "\n",
    "`df = pd.read_fwf('data.txt')  # for fixed-width files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xHNeWPeLxBXf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHNeWPeLxBXf",
    "outputId": "74a35170-461c-4ab4-809c-329e8aa1100f"
   },
   "outputs": [],
   "source": [
    "with open('Text File 3.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rSwqfW4Xx33t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "rSwqfW4Xx33t",
    "outputId": "46d82d75-35aa-4fcb-df28-06d1b7ebea50"
   },
   "outputs": [],
   "source": [
    "#Example:\n",
    "\n",
    "df = pd.read_fwf('Text File 3.txt')  # for fixed-width files\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T-UQc2nCx36_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-UQc2nCx36_",
    "outputId": "cba1b513-33d5-4b41-b59a-78505cc9b83f"
   },
   "outputs": [],
   "source": [
    "# Example TXT read\n",
    "txt_file_path = 'Text File 1.txt'  # Replace with your TXT file path\n",
    "with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:300])  # print first 300 characters\n",
    "\n",
    "with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "print(lines[:5])  # print first 5 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7DNhdXf6yb4u",
   "metadata": {
    "id": "7DNhdXf6yb4u"
   },
   "source": [
    "### 4. Python's built-in `open()` function\n",
    "- Methods: `read()` for entire content, `readlines()` for line-by-line reading.\n",
    "- Handle encoding if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57949de0-53e0-482c-8414-42d53d6b1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example TXT read\n",
    "txt_file_path = 'Text File 1.txt'  # Replace with your TXT file path\n",
    "with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:300])  # print first 300 characters\n",
    " \n",
    "with open(txt_file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "print(lines[:5])  # print first 5 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9Axw8GQzygA8",
   "metadata": {
    "id": "9Axw8GQzygA8"
   },
   "source": [
    "# 6. Writing DataFrames to Text Files\n",
    "\n",
    "### Key parameters:\n",
    "\n",
    "|Parameter|Purpose|\n",
    "|---------|-------|\n",
    "|`sep`|Defines the separator for the text file. You can choose tab (\\t), comma (,), or any other delimiter.|\n",
    "|`index=False`|Prevents writing row numbers (index) to the file.|\n",
    "|`header=True`|Writes the column names in the first row. It is True by default.|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-LLh-koix3-P",
   "metadata": {
    "id": "-LLh-koix3-P"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'Transaction_ID': [101, 102, 103, 104, 105],\n",
    "    'Customer_Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Monitor'],\n",
    "    'Amount': [1200, 800, 450, 200, 300],\n",
    "    'Date': ['2025-06-01', '2025-06-02', '2025-06-03', '2025-06-04', '2025-06-05']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n",
    "# Writing the DataFrame to a CSV file (which is a type of text file)\n",
    "df.to_csv('output.txt', sep='\\t', index=False)  # Tab-separated file\n",
    "\n",
    "# This will write the DataFrame as a text file using tab as separator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RC3u4z6JyteF",
   "metadata": {
    "id": "RC3u4z6JyteF"
   },
   "source": [
    "# 7. Reading JSON files\n",
    "\n",
    "JSON (JavaScript Object Notation) files are:\n",
    "- Text-based data formats used for storing and exchanging structured data.\n",
    "- Lightweight, human-readable, and widely used in web APIs, configuration files, and data storage.\n",
    "\n",
    "### Key parameters of `pd.read_json()`\n",
    "\n",
    "|Parameter|Description|\n",
    "|---------|-----------|\n",
    "|path_or_buf|\tThe file path, URL, or JSON string to read from.|\n",
    "|orient|Indicates the expected format of the JSON string. Examples: 'records', 'split', 'index'.|\n",
    "|typ|Type of object to return: 'frame' or 'series'.|\n",
    "|convert_dates|\tTry to convert string dates to datetime objects. (True by default).|\n",
    "|lines|\tSet to True if each line is a separate JSON object (i.e., JSON Lines format).|\n",
    "|dtype|\tAllows setting specific data types for columns.|\n",
    "|encoding|Specifies character encoding (e.g., 'utf-8').\t|\n",
    "\n",
    "\n",
    "### A typical JSON structure looks like this:\n",
    "\n",
    "```\n",
    "  [\n",
    "  {\n",
    "    \"PassengerId\": \"1\",\n",
    "    \"Name\": \"Braund, Mr. Owen Harris\",\n",
    "    \"Sex\": \"male\",\n",
    "    \"Survived\": \"0\"\n",
    "  },\n",
    "  {\n",
    "    \"PassengerId\": \"2\",\n",
    "    \"Name\": \"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",\n",
    "    \"Sex\": \"female\",\n",
    "    \"Survived\": \"1\"\n",
    "  },\n",
    "  {\n",
    "    \"PassengerId\": \"3\",\n",
    "    \"Name\": \"Heikkinen, Miss. Laina\",\n",
    "    \"Sex\": \"female\",\n",
    "    \"Survived\": \"1\"\n",
    "  },\n",
    "  {\n",
    "    \"PassengerId\": \"4\",\n",
    "    \"Name\": \"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",\n",
    "    \"Sex\": \"female\",\n",
    "    \"Survived\": \"1\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "### Example JSON read\n",
    "```\n",
    "json_file_path = 'sample_json.json'  # Replace with your JSON file path\n",
    "df_json = pd.read_json(json_file_path, orient='records', typ='frame', lines=False)\n",
    "print(df_json.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9jb6awLox4BF",
   "metadata": {
    "id": "9jb6awLox4BF"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\n",
    "    path_or_buf=\"titanic.json\",  # Path to the JSON file\n",
    "    orient=None,                 # Infers the structure format (used when not JSON Lines)\n",
    "    typ='frame',                 # Return a DataFrame (default is 'frame'; other option is 'series')\n",
    "    dtype=True,                  # Let pandas infer column data types\n",
    "    convert_axes=True,           # Convert axes to proper types (e.g., numeric index)\n",
    "    convert_dates=True,          # Automatically convert date strings to datetime\n",
    "    keep_default_dates=True,     # Also parse pandas-specific date formats\n",
    "    precise_float=False,         # Set to True for higher precision in floating point numbers\n",
    "    date_unit='ms',              # Use when converting numeric dates (ignored here, but good to know)\n",
    "    encoding='utf-8',            # Encoding used to decode the file\n",
    "    lines=True                   # Required for JSON Lines format (NDJSON)\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BBul_D0wy3ju",
   "metadata": {
    "id": "BBul_D0wy3ju"
   },
   "source": [
    "# 8. Writing DataFrames to JSON files\n",
    "\n",
    "### Key parameters of `df.to_json()`\n",
    "\n",
    "|Parameter|Description|\n",
    "|---------|-----------|\n",
    "|orient='records':| This specifies the format in which the DataFrame will be converted into JSON. In this case, records means that each row of the DataFrame will be converted into a dictionary, and the list of these dictionaries will be the JSON structure. It is useful for generating a JSON that represents each row of the DataFrame as an individual record, making it easy to use in applications that require record-based data, such as databases or APIs.|\n",
    "|lines=True:| This writes each record on a separate line in the JSON file. This option is helpful when you want to write large JSON files, where each line corresponds to a separate record. This structure is often easier to process in chunks, especially when dealing with large data.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GqxkNLODyzV0",
   "metadata": {
    "id": "GqxkNLODyzV0"
   },
   "outputs": [],
   "source": [
    "# Example DataFrame\n",
    "data = {\n",
    "    'Transaction_ID': [101, 102, 103, 104, 105],\n",
    "    'Customer_Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Monitor'],\n",
    "    'Amount': [1200, 800, 450, 200, 300],\n",
    "    'Date': ['2025-06-01', '2025-06-02', '2025-06-03', '2025-06-04', '2025-06-05']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "\n",
    "df.to_json('transaction_data.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jMPNNPNGzXqn",
   "metadata": {
    "id": "jMPNNPNGzXqn"
   },
   "source": [
    "# 9. Reading SQL tables\n",
    "- Use `pandas.read_sql()`.\n",
    "- Requires a DB connection and SQL query.\n",
    "- Common parameters: `sql`, `con`, `index_col`.\n",
    "\n",
    "|Parameter|Description|\n",
    "|---------|-----------|\n",
    "|sql|The SQL query or table name you want to read. This can be a SQL query (e.g., SELECT * FROM table_name) or the name of the table you want to load.|\n",
    "|con|The database connection object. This is typically a connection object created using a database adapter like sqlite3, pyodbc, SQLAlchemy, or other libraries depending on the database you are using.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "klaE-HHPRc6D",
   "metadata": {
    "id": "klaE-HHPRc6D"
   },
   "outputs": [],
   "source": [
    "#This code has been discussed in the session\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Open the .sql file and read the query\n",
    "with open('Create database.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "\n",
    "# Step 2: Connect to an SQLite database in memory (this will create a temporary database in memory)\n",
    "connection = sqlite3.connect(':memory:')\n",
    "\n",
    "# Step 3: Execute the SQL script to create the table and insert data\n",
    "connection.executescript(sql_script)\n",
    "\n",
    "# Step 4: Query the data and load it into a DataFrame\n",
    "#df = pd.read_sql(sql=\"SELECT * FROM users\", con=connection, index_col='id')\n",
    "df = pd.read_sql_query(\"SELECT * FROM users\", connection)\n",
    "\n",
    "# Step 5: Show the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the connection\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923778c1-299b-4647-b653-a9134eb575f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is for additional reference\n",
    "\n",
    "# Example: SQLite in-memory DB\n",
    "import sqlite3\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "conn = sqlite3.connect(':memory:')\n",
    "\n",
    "# Create a table\n",
    "conn.execute('CREATE TABLE sample_table (id INTEGER PRIMARY KEY, name TEXT, value REAL)')\n",
    "\n",
    "# Insert multiple rows\n",
    "conn.execute(\"INSERT INTO sample_table (name, value) VALUES ('Alice', 23.5), ('Bob', 45.2), ('Charlie', 31.8)\")\n",
    "\n",
    "# SQL query to read the data\n",
    "sql_query = 'SELECT * FROM sample_table'\n",
    "\n",
    "# Load the SQL result into a pandas DataFrame, using 'id' as index\n",
    "df_sql = pd.read_sql(sql=sql_query, con=conn, index_col='id')\n",
    "\n",
    "# Print the result\n",
    "print(df_sql)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jLKWcvdoze8O",
   "metadata": {
    "id": "jLKWcvdoze8O"
   },
   "source": [
    "# 10. Writing DataFrames to SQL tables\n",
    "\n",
    "### Key parameters of `df.to_sql()`\n",
    "\n",
    "|Parameter|Description|\n",
    "|---------|-----------|\n",
    "|name='transactions':| The name of the SQL table where the DataFrame will be written. Used to specify the table name where your data will be inserted.|\n",
    "|con=conn:| Purpose: The database connection. In this case, itâ€™s an SQLite connection (conn).| This connection object is used to interact with the database where the DataFrame will be written.|\n",
    "|index=False: | Prevents the index of the DataFrame from being written as a column in the SQL table. If you donâ€™t want the DataFrame index (row numbers) to be included in the SQL table as a column, set index=False.|\n",
    "|if_exists='replace':| Determines what to do if the table already exists. Why itâ€™s important: This controls how the table will be handled if it already exists. You can choose to replace, append, or fail based on your use case. <br> â€¢ replace: Drop the table if it exists and create a new one. <br> â€¢ append: Append data to the existing table. <br> â€¢ fail: Raise an error if the table already exists. <br>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "It8hUZQkRfJx",
   "metadata": {
    "id": "It8hUZQkRfJx"
   },
   "outputs": [],
   "source": [
    "#This code has been discussed in the session\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dummy transactional dataset\n",
    "data = {\n",
    "    'Transaction_ID': [101, 102, 103, 104, 105],\n",
    "    'Customer_Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Monitor'],\n",
    "    'Amount': [1200, 800, 450, 200, 300],\n",
    "    'Date': ['2025-06-01', '2025-06-02', '2025-06-03', '2025-06-04', '2025-06-05']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create the .sql file and write SQL commands\n",
    "\n",
    "with open('transaction_data.sql', 'w') as file:\n",
    "    # Write the CREATE TABLE statement\n",
    "    file.write(\"CREATE TABLE IF NOT EXISTS transactions (\\n\")\n",
    "    file.write(\"    Transaction_ID INTEGER PRIMARY KEY,\\n\")\n",
    "    file.write(\"    Customer_Name TEXT,\\n\")\n",
    "    file.write(\"    Product TEXT,\\n\")\n",
    "    file.write(\"    Amount INTEGER,\\n\")\n",
    "    file.write(\"    Date TEXT\\n\")\n",
    "    file.write(\");\\n\\n\")\n",
    "\n",
    "    # Write the INSERT INTO statements for each row in the DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        file.write(f\"INSERT INTO transactions (Transaction_ID, Customer_Name, Product, Amount, Date) VALUES ({row['Transaction_ID']}, '{row['Customer_Name']}', '{row['Product']}', {row['Amount']}, '{row['Date']}');\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1f5f8-f1b8-4a81-a7fa-ff99e2972d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is for additional reference\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Create a dummy transactional dataset\n",
    "data = {\n",
    "    'Transaction_ID': [101, 102, 103, 104, 105],\n",
    "    'Customer_Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Headphones', 'Monitor'],\n",
    "    'Amount': [1200, 800, 450, 200, 300],\n",
    "    'Date': ['2025-06-01', '2025-06-02', '2025-06-03', '2025-06-04', '2025-06-05']\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create an SQLite database in memory\n",
    "conn = sqlite3.connect('transaction_data.db')\n",
    "\n",
    "# Write the DataFrame to an SQL table\n",
    "df.to_sql('transactions', conn, index=False, if_exists='replace')\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hRwwiq9wzvDi",
   "metadata": {
    "id": "hRwwiq9wzvDi"
   },
   "source": [
    "# 11. Reading HTML tables\n",
    "\n",
    "HTML tables are an essential part of web pages used to display tabular data in a structured format. In HTML, a table is defined using specific HTML tags that organize data into rows and columns. Each table can consist of several components, such as headers, rows, and cells, which provide clarity and structure to the data being presented.\n",
    "\n",
    "Structure of an HTML Table\n",
    "The basic structure of an HTML table involves the following key tags:\n",
    "\n",
    "- **`<table>`**: Defines the table itself.\n",
    "  \n",
    "- **`<tr>`**: Defines a table row.\n",
    "\n",
    "- **`<th>`**: Defines a table header cell. Text inside `<th>` is typically bold and centered.\n",
    "\n",
    "- **`<td>`**: Defines a table data cell. These cells contain the actual data in the table.\n",
    "\n",
    "- **`<thead>`**: Defines the header section of a table (optional).\n",
    "\n",
    "- **`<tbody>`**: Defines the body section of the table (optional).\n",
    "\n",
    "- **`<tfoot>`**: Defines the footer section of a table (optional).\n",
    "\n",
    "## Example of an HTML Table:\n",
    "Hereâ€™s an example of an HTML table that contains data about products and their prices:\n",
    "\n",
    "```\n",
    "html\n",
    "Copy\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Product</th>\n",
    "      <th>Price</th>\n",
    "      <th>Quantity</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Laptop</td>\n",
    "      <td>$1000</td>\n",
    "      <td>5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Phone</td>\n",
    "      <td>$500</td>\n",
    "      <td>10</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Tablet</td>\n",
    "      <td>$300</td>\n",
    "      <td>15</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "  <tfoot>\n",
    "    <tr>\n",
    "      <td>Total</td>\n",
    "      <td>$1800</td>\n",
    "      <td>30</td>\n",
    "    </tr>\n",
    "  </tfoot>\n",
    "</table>\n",
    "```\n",
    "## Explanation of the Table Components:\n",
    "- **`<table>`**: Defines the entire table.\n",
    "\n",
    "- **`<thead>`**: Contains the header row with column names (Product, Price, Quantity).\n",
    "\n",
    "- **`<tbody>`**: Contains the body of the table where the actual data resides.\n",
    "\n",
    "- **`<tfoot>`**: Typically used to display totals or summaries for the table.\n",
    "\n",
    "## Styling HTML Tables:\n",
    "HTML tables can be styled using CSS (Cascading Style Sheets) to enhance the visual appearance. For example:\n",
    "\n",
    "```\n",
    "html\n",
    "Copy\n",
    "<style>\n",
    "  table {\n",
    "    width: 50%;\n",
    "    border-collapse: collapse;\n",
    "  }\n",
    "  th, td {\n",
    "    padding: 10px;\n",
    "    border: 1px solid black;\n",
    "    text-align: center;\n",
    "  }\n",
    "  th {\n",
    "    background-color: #f2f2f2;\n",
    "  }\n",
    "</style>\n",
    "```\n",
    "\n",
    "This will create a table with a 50% width, padding, and borders for each cell, and the header row will have a light background color.\n",
    "\n",
    "## Use Cases for HTML Tables:\n",
    "- Data Representation: Displaying structured data, such as product lists, financial statements, or contact lists.\n",
    "- Form Layout: Organizing form input fields in a clean and accessible way.\n",
    "- Reports and Dashboards: Presenting summarized or detailed data in an easily readable format.\n",
    "\n",
    "## Key Points:\n",
    "- HTML tables are widely used for displaying structured data in rows and columns on web pages.\n",
    "- They provide a simple way to represent tabular information that can be easily read and understood.\n",
    "- HTML tables can be styled and customized using CSS to create visually appealing layouts.\n",
    "- `pd.read_html()` is a powerful function for extracting HTML tables directly into a DataFrame.\n",
    "- Parameters like `match`, `attrs`, and `header` allow fine control over which tables to extract and how to interpret the data.\n",
    "- Parameters like `skiprows`, `index_col`, and `parse_dates` help in managing the table structure and data types.\n",
    "\n",
    "### Key parameters of `pd.read_html()`\n",
    "\n",
    "|Parameter|Description|\n",
    "|---------|-----------|\n",
    "|io:| Type: String or file-like object <br> Purpose: The URL or file path to read the HTML data from. It can also be an HTML string or file object. <br> Explanation: This is where you specify the location of the HTML file or URL containing the table(s). For example, you can pass a URL like 'http://example.com', or a local file path like 'path_to_file.html'.|\n",
    "|match:|Type: String (optional) <br> Purpose: A regular expression to match table tags. <br> Explanation: You can use this parameter to filter which tables to extract from the HTML file based on a pattern. By default, match=None, which extracts all the tables from the HTML. For example, if you are looking for a specific table with the class name data-table, you can use match=\"data-table\" to filter it.|\n",
    "|attrs:| Type: Dictionary (optional) <br> Purpose: Filters tables based on specific attributes like class or id. <br> Explanation: This parameter is useful when you want to read a table with a specific id or class. For example, attrs={'class': 'data-table'} will only extract tables with the class attribute set to 'data-table'.|\n",
    "|header:| Type: Integer, list of integers, or None (optional) <br> Purpose: Specifies which row(s) to use as the header of the table. <br> Explanation: This parameter defines the row number(s) to use as the header for the DataFrame. If set to 0 (default), the first row is used as the header. If you want to use multiple rows as headers, pass a list like [0, 1]. Set it to None if you want Pandas to treat the first row as data instead.|\n",
    "|index_col:| Type: Integer, string, or None (optional) <br> Purpose: Specifies which column(s) to set as the index of the DataFrame. <br> Explanation: This allows you to set one or more columns from the table as the index of the resulting DataFrame. For example, if you want the first column to be the index, set index_col=0.|\n",
    "|skiprows:| Type: Integer or list of integers (optional) <br> Purpose: Specifies how many rows to skip at the beginning. <br> Explanation: This parameter is useful if there are irrelevant rows before the table data. For example, if the table starts after the first two rows, you can set skiprows=2.|\n",
    "|skip_footer:| Type: Integer (optional) <br> Purpose: Specifies how many rows to skip at the end of the table. <br> Explanation: This parameter is useful when the table has extra rows at the end that are not needed (e.g., footnotes, totals, etc.).|\n",
    "|parse_dates:| Type: List of integers or booleans (optional) <br> Purpose: Specifies which columns to parse as dates. <br> Explanation: If the table contains columns with date values, you can use this parameter to automatically parse those columns into datetime objects. For example, parse_dates=[0, 1] would convert the first and second columns into date format.|\n",
    "|thousands:| Type: String (optional) <br> Purpose: Specifies the thousands separator for parsing numbers. <br> Explanation: If the numeric data contains thousands separators (e.g., commas in the US or dots in Europe), you can specify that separator using this parameter. For example, use thousands=',' to correctly parse numbers like 1,000 into 1000.|\n",
    "|encoding:| Type: String (optional) <br> Purpose: Specifies the character encoding to use for the HTML file. <br> Explanation: This parameter is useful when working with files that contain non-ASCII characters. For example, you can use encoding='utf-8' to ensure that characters are correctly decoded.|\n",
    "|flavor:| Type: String (optional) <br> Purpose: Specifies the flavor of parsing to use, either 'lxml' or 'html5lib'. <br> Explanation: Pandas uses lxml by default, but if you encounter parsing issues, you can specify the html5lib parser, which can sometimes be more lenient in reading poorly formatted HTML.|\n",
    "|parse_nums:| Type: Boolean (optional) <br> Purpose: Automatically parses columns that look like numeric data. <br> Explanation: Set to True if you want Pandas to try to convert columns that appear to contain numeric data into actual numeric types.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L8KWB9F8zmMi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8KWB9F8zmMi",
    "outputId": "334a040e-dddf-4a5e-f82e-6f6780e5ca35"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_Netflix_original_films_(2024)\"\n",
    "\n",
    "# Use pandas.read_html to extract all tables from the page\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "# Check how many tables were foundâ€”usually the main film list is among them\n",
    "print(f\"Found {len(tables)} tables on the page.\")\n",
    "\n",
    "# Pick the most likely table (often the first one; adjust index if needed)\n",
    "df = tables[0]\n",
    "\n",
    "# Inspect the first few rows to verify youâ€™ve got the correct table\n",
    "print(df.head())\n",
    "\n",
    "# Normalize column names: convert everything to strings and strip whitespace\n",
    "df.columns = [str(col).strip() for col in df.columns]\n",
    "\n",
    "# Clean up any odd column names, renaming for clarity (adjust names as needed)\n",
    "df = df.rename(columns={\n",
    "    'Title': 'Film_Title',\n",
    "    'Release date': 'Release_Date',\n",
    "    'Genre(s)': 'Genres',\n",
    "    'Director(s)': 'Directors',\n",
    "    # Add more renames based on actual header names\n",
    "})\n",
    "\n",
    "# (Optional) Convert release date column to datetime for easier filtering/sorting\n",
    "df['Release_Date'] = pd.to_datetime(df['Release_Date'], errors='coerce', dayfirst=True)\n",
    "\n",
    "# ðŸŽ‰ Final: `df` now holds the Netflix films list in a tidy, pandas-ready format\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kEoY8dlkz6Zr",
   "metadata": {
    "id": "kEoY8dlkz6Zr"
   },
   "source": [
    "# 12. Reading Data Using API\n",
    "\n",
    "APIs (Application Programming Interfaces) are sets of protocols, routines, and tools that allow different software applications to communicate with each other. An API defines the methods and data formats that applications can use to request and exchange information with each other.\n",
    "\n",
    "## Key Points About APIs:\n",
    "- Interface for Communication: An API acts as an intermediary that enables different systems or software components to interact with each other. For example, a weather application might use an API to get weather data from a remote server.\n",
    "- Abstraction: APIs abstract the internal workings of a system, providing a simple interface for developers to use without needing to understand how the underlying system works.\n",
    "- Request-Response Model:\n",
    "Typically, an API follows a request-response model, where one system sends a request for data or action, and the other system responds with the requested data or confirmation of action.\n",
    "- Common Use Cases:\n",
    "APIs are widely used to integrate services such as payment gateways (Stripe, PayPal), social media (Twitter, Facebook), weather services, mapping services (Google Maps), and more.\n",
    "\n",
    "## Types of APIs:\n",
    "\n",
    "1. Web APIs: These are APIs that are accessible over the web (HTTP/HTTPS), and they are commonly used to allow integration between different web services.\n",
    "  - Example: REST API, SOAP API, GraphQL.\n",
    "\n",
    "2. Library/API in Programming Languages: These are APIs that are available in programming languages like Python, Java, and JavaScript. They provide functions and libraries that programmers can use to interact with the underlying system or perform specific tasks.\n",
    "  - Example: The Python requests library to send HTTP requests.\n",
    "\n",
    "3. Operating System APIs: These provide an interface to the functionalities of an operating system\n",
    "  - Example: Windows API, POSIX API.\n",
    "\n",
    "4. Database APIs: Allow applications to interact with a database system for tasks like retrieving, updating, and deleting data.\n",
    "  - Example: SQL-based APIs for interacting with relational databases like MySQL, PostgreSQL.\n",
    "\n",
    "## API Methods/Requests:\n",
    "Common HTTP methods used in Web APIs include:\n",
    "- GET: Fetch data from the server (e.g., get user data).\n",
    "- POST: Send data to the server (e.g., create a new user).\n",
    "- PUT: Update existing data on the server (e.g., update user profile).\n",
    "- DELETE: Remove data from the server (e.g., delete a user account)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MRsyZcy3zmP2",
   "metadata": {
    "id": "MRsyZcy3zmP2"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_weather(lat, lon, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Fetch daily weather data (temperature, wind speed) from Open-Meteo.\n",
    "\n",
    "    Parameters:\n",
    "      lat (float): latitude\n",
    "      lon (float): longitude\n",
    "      start_date (str): in 'YYYY-MM-DD' format (optional)\n",
    "      end_date (str): in 'YYYY-MM-DD' format (optional)\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: daily time series of weather variables\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"daily\": \"temperature_2m_max,temperature_2m_min,windspeed_10m_max\",\n",
    "        \"timezone\": \"UTC\"\n",
    "    }\n",
    "    if start_date:\n",
    "        params[\"start_date\"] = start_date\n",
    "    if end_date:\n",
    "        params[\"end_date\"] = end_date\n",
    "\n",
    "    resp = requests.get(base_url, params=params)\n",
    "    resp.raise_for_status()\n",
    "    js = resp.json()\n",
    "\n",
    "    df = pd.DataFrame(js[\"daily\"])\n",
    "    return df\n",
    "\n",
    "# Example: Bengaluru weather for next 7 days\n",
    "df = get_weather(12.9716, 77.5946)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n2PlZmRT0ObH",
   "metadata": {
    "id": "n2PlZmRT0ObH"
   },
   "source": [
    "# 13. Web Scraping HTML Table\n",
    "\n",
    "# Step 1: Import necessary libraries\n",
    "\n",
    "- requests: This library is used to send HTTP requests to retrieve web page content. It allows interaction with web resources.\n",
    "\n",
    "- BeautifulSoup from bs4: This library helps parse HTML and XML documents. Itâ€™s commonly used for web scraping.\n",
    "\n",
    "- pandas: A powerful data manipulation and analysis library used to work with structured data, such as tables, CSV, Excel, or SQL databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Alr3VlakzmTB",
   "metadata": {
    "id": "Alr3VlakzmTB"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4did5ctw6pj-",
   "metadata": {
    "id": "4did5ctw6pj-"
   },
   "source": [
    "# Step 2: Get the page content\n",
    "\n",
    "- `url`: The URL of the Wikipedia page that contains the table of Netflix films in 2024.\n",
    "\n",
    "- `requests.get(url)`: Sends an HTTP GET request to retrieve the content of the page.\n",
    "\n",
    "- `response.text`: Extracts the HTML content from the response object.\n",
    "\n",
    "- `BeautifulSoup(response.text, 'html.parser')`: Parses the raw HTML text of the page using BeautifulSoup and the html.parser to structure it into a BeautifulSoup object (soup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DKadukhu3Zse",
   "metadata": {
    "id": "DKadukhu3Zse"
   },
   "outputs": [],
   "source": [
    "# Step 2: Get the page content\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_Netflix_original_films_(2024)\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yP9s3Ipm8VqM",
   "metadata": {
    "id": "yP9s3Ipm8VqM"
   },
   "source": [
    "# Step 3: Find the first table on the page\n",
    "\n",
    "- `soup.find()`: This method searches the parsed HTML (soup) and returns the first matching element.\n",
    "\n",
    "- `'table'`: We're looking for the **'< table >'** element.\n",
    "\n",
    "- `{'class': 'wikitable'}`: Specifies that we want a table with the class wikitable. Wikipedia tables often use this class for structured data tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MU3uyWZ_zmYY",
   "metadata": {
    "id": "MU3uyWZ_zmYY"
   },
   "outputs": [],
   "source": [
    "# Step 3: Find the first table on the page\n",
    "table = soup.find('table', {'class': 'wikitable'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccmu70x8-Y4L",
   "metadata": {
    "id": "ccmu70x8-Y4L"
   },
   "source": [
    "# Step 4: Extract headers\n",
    "\n",
    "- `headers = []`: Initializes an empty list to store the table headers.\n",
    "\n",
    "- `table.find_all('th')`: Finds all <th> (table header) elements in the table.\n",
    "\n",
    "- `th.text.strip()`: Extracts the text from each header (th), removes extra spaces around the text using strip(), and appends it to the headers list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NThFbv62zmcV",
   "metadata": {
    "id": "NThFbv62zmcV"
   },
   "outputs": [],
   "source": [
    "# Step 4: Extract headers\n",
    "headers = []\n",
    "for th in table.find_all('th'):\n",
    "    headers.append(th.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PnhSvj87-aL8",
   "metadata": {
    "id": "PnhSvj87-aL8"
   },
   "source": [
    "# Step 5: Extract rows\n",
    "\n",
    "- `rows = []`: Initializes an empty list to store the rows of data.\n",
    "\n",
    "- `table.find_all('tr')[1:]`: Finds all <tr> (table row) elements, but skips the first one (header row) by slicing [1:].\n",
    "\n",
    "- `tr.find_all(['td', 'th'])`: Finds all table data (<td>) and header (<th>) cells within each row (tr).\n",
    "\n",
    "- `[cell.text.strip() for cell in cells]`: Extracts the text from each cell (cell.text) and strips any leading or trailing whitespace.\n",
    "\n",
    "- `if len(row) == len(headers):`: Checks if the number of cells in the row matches the number of headers. If not, that row is discarded.\n",
    "\n",
    "- `rows.append(row)`: Adds the valid row to the rows list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F5Ala0gox4Gp",
   "metadata": {
    "id": "F5Ala0gox4Gp"
   },
   "outputs": [],
   "source": [
    "# Step 5: Extract rows\n",
    "rows = []\n",
    "for tr in table.find_all('tr')[1:]:  # Skip header\n",
    "    cells = tr.find_all(['td', 'th'])\n",
    "    row = [cell.text.strip() for cell in cells]\n",
    "    if len(row) == len(headers):  # Only keep rows with expected number of columns\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9H2dUfpz-d4m",
   "metadata": {
    "id": "9H2dUfpz-d4m"
   },
   "source": [
    "# Step 6: Create a DataFrame\n",
    "\n",
    "`pd.DataFrame(rows, columns=headers)`: Creates a Pandas DataFrame from the list of rows (rows) and uses the list of headers (headers) for the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gjBGzp6Ax4MI",
   "metadata": {
    "id": "gjBGzp6Ax4MI"
   },
   "outputs": [],
   "source": [
    "# Step 6: Create a DataFrame\n",
    "df = pd.DataFrame(rows, columns=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hp95_Kwz-e1h",
   "metadata": {
    "id": "hp95_Kwz-e1h"
   },
   "source": [
    "# Step 7: Clean column names (optional)\n",
    "\n",
    "- `df.columns`: Accesses the column names of the DataFrame.\n",
    "\n",
    "- `col.replace('\\xa0', ' ')`: Replaces the non-breaking space character (\\xa0) with a regular space (' '), ensuring consistent column naming.\n",
    "\n",
    "- `strip()`: Removes any leading or trailing whitespace from the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LfwuEn6XxBaN",
   "metadata": {
    "id": "LfwuEn6XxBaN"
   },
   "outputs": [],
   "source": [
    "# Step 7: Clean column names (optional)\n",
    "df.columns = [col.replace('\\xa0', ' ').strip() for col in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ekuAWxsE-ffR",
   "metadata": {
    "id": "ekuAWxsE-ffR"
   },
   "source": [
    "# Step 8: Rename for consistency (optional)\n",
    "\n",
    "- `df.rename(columns={...})`: Renames specific columns for consistency and better readability.\n",
    "\n",
    "- For example, the column 'Title' is renamed to 'Film_Title', 'Release date' to 'Release_Date', and so on.\n",
    "\n",
    "- This ensures that column names are uniform and meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aUYzQLnX75gi",
   "metadata": {
    "id": "aUYzQLnX75gi"
   },
   "outputs": [],
   "source": [
    "# Step 8: Rename for consistency (optional)\n",
    "df = df.rename(columns={\n",
    "    'Title': 'Film_Title',\n",
    "    'Release date': 'Release_Date',\n",
    "    'Genre(s)': 'Genres',\n",
    "    'Director(s)': 'Directors',\n",
    "    # Add more as needed\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YvJRbjdX-gca",
   "metadata": {
    "id": "YvJRbjdX-gca"
   },
   "source": [
    "# Step 9: Convert date column to datetime (optional)\n",
    "\n",
    "`df['Release_Date']`: Accesses the Release_Date column in the DataFrame.\n",
    "\n",
    "`pd.to_datetime(df['Release_Date'], errors='coerce', dayfirst=True)`: Converts the Release_Date column to a Pandas datetime object.\n",
    "\n",
    "`errors='coerce'`: If any date is invalid, it will be set to NaT (Not a Time) instead of throwing an error.\n",
    "\n",
    "`dayfirst=True`: Ensures that the day comes before the month in the date format (common in many countries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90qBvquh75on",
   "metadata": {
    "id": "90qBvquh75on"
   },
   "outputs": [],
   "source": [
    "# Step 9: Convert date column to datetime (optional)\n",
    "df['Release_Date'] = pd.to_datetime(df['Release_Date'], errors='coerce', dayfirst=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IbhitRoV-hgj",
   "metadata": {
    "id": "IbhitRoV-hgj"
   },
   "source": [
    "`df.info()`: Displays a summary of the DataFrame, including the number of entries, column names, and data types.\n",
    "\n",
    "`df.head()`: Displays the first 5 rows of the DataFrame to check the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cw0Snr-75rr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cw0Snr-75rr",
    "outputId": "1e3ef87e-1ead-4f42-dff7-fac569eff367"
   },
   "outputs": [],
   "source": [
    "# Display info\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O7AEGNd675vM",
   "metadata": {
    "id": "O7AEGNd675vM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
